# I/O Request Latency (iolat) — Week 2 Design & Validation

This document describes the design, implementation, and validation of the
`iolat` module, which measures **block I/O request latency** using eBPF.

The goal of this module is to:
- measure end-to-end latency of block-layer I/O requests,
- apply a correct **object-based keying strategy**,
- and unify observability UX with other latency modules (`memlat`, `runqlat`).

This document is written for interviewers and engineers evaluating
practical kernel observability and performance measurement skills.

---

## Measurement Goal

Measure block I/O request latency:

Δt = t_complete - t_issue

where:
- `t_issue`    is the timestamp when an I/O request is issued to the block layer,
- `t_complete` is the timestamp when the request is completed.

The resulting latency is aggregated into a histogram to observe:
- typical fast-path I/O,
- and rare but important high-latency tail behavior.

---

## Probe Selection

### Target Tracepoints

The Linux block layer exposes tracepoints that naturally define
the lifetime of an I/O request.

- `block:block_rq_issue`
- `block:block_rq_complete`

These tracepoints mark:
- the moment a request enters the block layer,
- and the moment it finishes (success or error).

This pair provides a clean and stable boundary for I/O latency measurement
without requiring driver-specific instrumentation.

---

## Key Design

### Why `struct request *` as the Key?

Unlike CPU or memory faults, I/O requests are **not naturally bound to a single thread**.
An I/O request may:
- be issued by one process,
- scheduled and merged internally,
- and completed asynchronously.

Therefore, using `pid` or `pid_tgid` as the key would be incorrect.

Instead, the key is:

key = (u64) request_pointer


Rationale:
- `struct request *` uniquely identifies a block I/O request.
- The pointer remains stable for the lifetime of the request.
- This avoids collisions and ensures correct matching of issue/completion events.

---

## Map Design

Two eBPF maps are used.

### 1) start_ts (HASH)

Stores per-request metadata captured at issue time.

| Field | Description |
|-----|-------------|
| Key | `rq_id` (casted `struct request *`) |
| Value | `struct iolat_start` |

`struct iolat_start` contains:
- issue timestamp (nanoseconds),
- issuing process `pid_tgid`,
- issuing process `comm`.

This extra metadata is not required for latency calculation,
but significantly improves debugging and future analysis.

---

### 2) hist (ARRAY)

Latency histogram aggregated in kernel space.

| Field | Description |
|-----|-------------|
| Key | bucket index |
| Value | count |

Bucketization uses a logarithmic scale:

delta_us = (now_ns - start_ns) / 1000
bucket = log2(delta_us)

This compact representation highlights tail latency
without excessive memory usage.

---

## Kernel-Space Logic (eBPF)

### Issue Probe (`block_rq_issue`)

1. Compute request key: `(u64)rq`
2. Capture:
   - current timestamp
   - `pid_tgid`
   - `comm`
3. Store metadata in `start_ts[rq_id]`

---

### Complete Probe (`block_rq_complete`)

1. Lookup `start_ts[rq_id]`
2. If missing:
   - skip (valid case: tracing started mid-flight)
3. Compute:
   - `delta = now - start`
   - convert to microseconds
4. Bucketize and increment `hist[bucket]`
5. Delete `start_ts[rq_id]`

Deleting the entry guarantees:
- bounded map size,
- no stale request state.

---

## User-Space Responsibilities (Go)

The Go user-space program:
- loads the eBPF object generated by `bpf2go`,
- attaches BTF-based tracepoints,
- controls collection duration,
- reads histogram data,
- prints a human-readable histogram,
- saves results to disk.

The output format is unified across all modules:

### CSV
bucket,lo_us,hi_us,count


### Summary JSON
Includes:
- module name
- metric name
- duration
- total events
- tail events
- maximum observed bucket

This unified schema enables:
- one-click collection,
- automated comparison,
- consistent analysis tooling.

---

## Validation Strategy

### Why Validation Matters

Latency metrics can appear “reasonable” even when incorrect.
Validation ensures that:

> the metric reacts predictably to real I/O workload changes.

---

### Validation Workload

Block I/O activity was generated implicitly by normal system activity
and verified by repeated collection runs.

(Planned extension: `fio`-based controlled read/write workloads.)

---

### Observed Behavior

- Most I/O requests fall within low-latency buckets (tens of microseconds).
- A small but non-zero tail appears in higher buckets,
  confirming asynchronous and device-dependent behavior.
- The histogram shape differs clearly from `memlat` and `runqlat`,
  indicating that each module measures a distinct subsystem.

---

## Key Takeaways

- I/O latency must be keyed by **request objects**, not processes.
- Tracepoint pairing provides a clean, low-overhead measurement boundary.
- Histogram-based analysis exposes rare but impactful slow I/O events.
- Metadata capture at issue time greatly improves observability value.

---

## Next Steps

- Run controlled experiments using `fio` to stress:
  - sequential reads,
  - random writes,
  - queue depth variation.
- Correlate I/O latency tail with:
  - CPU run queue latency,
  - memory pressure.
- Extend summary reports for cross-module analysis.

---
